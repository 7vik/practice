{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import regex as re\n",
    "import requests\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding is simply converting text into numbers so they can be fed into a neural network\n",
    "# the first thing we need is a corpus of text to train our encoding\n",
    "\n",
    "corpus = [\n",
    "    \"You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings.\",\n",
    "    \"I arrived here yesterday, and my first task is to assure my dear sister of my welfare and increasing confidence in the success of my undertaking.\",\n",
    "    \"I am already far north of London, and as I walk in the streets of Petersburgh, I feel a cold northern breeze play upon my cheeks, which braces my nerves and fills me with delight.\",\n",
    "    \"Do you understand this feeling?\",\n",
    "    \"This breeze, which has travelled from the regions towards which I am advancing, gives me a foretaste of those icy climes.\",\n",
    "    \"Inspirited by this wind of promise, my daydreams become more fervent and vivid.\",\n",
    "    \"I try in vain to be persuaded that the pole is the seat of frost and desolation; it ever presents itself to my imagination as the region of beauty and delight.\",\n",
    "    \"There, Margaret, the sun is forever visible, its broad disk just skirting the horizon and diffusing a perpetual splendour.\",\n",
    "    \"There—for with your leave, my sister, I will put some trust in preceding navigators—there snow and frost are banished; and, sailing over a calm sea, we may be wafted to a land surpassing in wonders and in beauty every region hitherto discovered on the habitable globe.\",\n",
    "    \"Its productions and features may be without example, as the phenomena of the heavenly bodies undoubtedly are in those undiscovered solitudes.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'You': 1, 'will': 2, 'rejoice': 1, 'to': 5, 'hear': 1, 'that': 2, 'no': 1, 'disaster': 1, 'has': 2, 'accompanied': 1, 'the': 12, 'commencement': 1, 'of': 10, 'an': 1, 'enterprise': 1, 'which': 4, 'you': 2, 'have': 1, 'regarded': 1, 'with': 3, 'such': 1, 'evil': 1, 'forebodings.': 1, 'I': 7, 'arrived': 1, 'here': 1, 'yesterday,': 1, 'and': 11, 'my': 9, 'first': 1, 'task': 1, 'is': 3, 'assure': 1, 'dear': 1, 'sister': 1, 'welfare': 1, 'increasing': 1, 'confidence': 1, 'in': 7, 'success': 1, 'undertaking.': 1, 'am': 2, 'already': 1, 'far': 1, 'north': 1, 'London,': 1, 'as': 3, 'walk': 1, 'streets': 1, 'Petersburgh,': 1, 'feel': 1, 'a': 5, 'cold': 1, 'northern': 1, 'breeze': 1, 'play': 1, 'upon': 1, 'cheeks,': 1, 'braces': 1, 'nerves': 1, 'fills': 1, 'me': 2, 'delight.': 2, 'Do': 1, 'understand': 1, 'this': 2, 'feeling?': 1, 'This': 1, 'breeze,': 1, 'travelled': 1, 'from': 1, 'regions': 1, 'towards': 1, 'advancing,': 1, 'gives': 1, 'foretaste': 1, 'those': 2, 'icy': 1, 'climes.': 1, 'Inspirited': 1, 'by': 1, 'wind': 1, 'promise,': 1, 'daydreams': 1, 'become': 1, 'more': 1, 'fervent': 1, 'vivid.': 1, 'try': 1, 'vain': 1, 'be': 3, 'persuaded': 1, 'pole': 1, 'seat': 1, 'frost': 2, 'desolation;': 1, 'it': 1, 'ever': 1, 'presents': 1, 'itself': 1, 'imagination': 1, 'region': 2, 'beauty': 2, 'There,': 1, 'Margaret,': 1, 'sun': 1, 'forever': 1, 'visible,': 1, 'its': 1, 'broad': 1, 'disk': 1, 'just': 1, 'skirting': 1, 'horizon': 1, 'diffusing': 1, 'perpetual': 1, 'splendour.': 1, 'There—for': 1, 'your': 1, 'leave,': 1, 'sister,': 1, 'put': 1, 'some': 1, 'trust': 1, 'preceding': 1, 'navigators—there': 1, 'snow': 1, 'are': 2, 'banished;': 1, 'and,': 1, 'sailing': 1, 'over': 1, 'calm': 1, 'sea,': 1, 'we': 1, 'may': 2, 'wafted': 1, 'land': 1, 'surpassing': 1, 'wonders': 1, 'every': 1, 'hitherto': 1, 'discovered': 1, 'on': 1, 'habitable': 1, 'globe.': 1, 'Its': 1, 'productions': 1, 'features': 1, 'without': 1, 'example,': 1, 'phenomena': 1, 'heavenly': 1, 'bodies': 1, 'undoubtedly': 1, 'undiscovered': 1, 'solitudes.': 1})\n"
     ]
    }
   ],
   "source": [
    "# there's a pre-tokenization step where we compute the frequency of each word in the corpus and get the alphabet\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "\n",
    "for text in corpus:\n",
    "    for word in text.split():\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "print(word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = sorted(list(set(letter for word in word_freqs for letter in word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",|.|;|?|D|I|L|M|P|T|Y|a|b|c|d|e|f|g|h|i|j|k|l|m|n|o|p|r|s|t|u|v|w|x|y|z|—\n"
     ]
    }
   ],
   "source": [
    "print(*alphabet, sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {word: [c for c in word] for word in word_freqs.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('r', 'e'): 29\n",
      "('h', 'e'): 23\n",
      "('e', 'a'): 12\n",
      "('a', 'r'): 11\n",
      "('t', 'h'): 26\n",
      "('i', 's'): 16\n",
      "('a', 's'): 11\n",
      "('s', 't'): 12\n",
      "('e', 'r'): 25\n",
      "('a', 'n'): 18\n",
      "('e', 'd'): 12\n",
      "('v', 'e'): 14\n",
      "('d', 'e'): 11\n",
      "('i', 't'): 11\n",
      "('i', 'n'): 21\n",
      "('e', 's'): 11\n",
      "('n', 'd'): 22\n",
      "('o', 'n'): 13\n"
     ]
    }
   ],
   "source": [
    "pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "for i, key in enumerate(pair_freqs.keys()):\n",
    "    if pair_freqs[key] > 10:\n",
    "        print(f\"{key}: {pair_freqs[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('r', 'e') 29\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair, freq in pair_freqs.items():\n",
    "    if max_freq is None or max_freq < freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "\n",
    "print(best_pair, max_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < (len(split) - 1):\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............"
     ]
    }
   ],
   "source": [
    "vocab_size = 50\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    print('.', end=\"\")\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {('r', 'e'): 're', ('t', 'h'): 'th', ('n', 'd'): 'nd', ('i', 'n'): 'in', ('e', 'r'): 'er', ('i', 's'): 'is', ('a', 'nd'): 'and', ('th', 'e'): 'the', ('a', 's'): 'as', ('o', 'n'): 'on', ('e', 'd'): 'ed', ('o', 'f'): 'of'})\n"
     ]
    }
   ],
   "source": [
    "print(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', ';', '?', 'D', 'I', 'L', 'M', 'P', 'T', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', 're', 'th', 'nd', 'in', 'er', 'is', 'and', 'the', 'as', 'on', 'ed', 'of']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    word_split = text.split(' ')\n",
    "    splits = [[l for l in word] for word in word_split]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', 'h', 'is', 'is', 'n', 'o', 't', 'the', 't', 'o', 'k', 'e', 'n', '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is not the token.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kihopark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
